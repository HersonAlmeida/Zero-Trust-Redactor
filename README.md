# ğŸ”’ Zero-Trust Redactor Pro

**Localhost Privacy Suite** - Offline-first PII redaction with local AI. No cloud uploads for your documents.

A desktop-first application for redacting Personally Identifiable Information (PII) from documents. AI inference runs locally on your machineâ€”your document data never leaves your device. One-time model/library downloads come from trusted CDNs (Hugging Face, jsDelivr) during setup.

## âœ¨ Features

- **ğŸš€ Fast Mode** - BERT NER + Regex patterns for instant detection
- **ğŸ§  Deep Scan** - Llama 3.2 1B for context-aware PII detection  
- **ğŸ“„ PDF Layout Preservation** - Redactions maintain original document formatting
- **âœ‹ Manual Selection** - Highlight text to add to redaction list
- **ğŸ”’ Zero Trust** - No document data leaves your device; network only for one-time model/library downloads
- **ğŸ“‹ Report Generation** - Export detection reports as PDF

## ğŸš€ Quick Start

### Prerequisites

- Node.js 18+ 
- Python 3.9+
- ~2GB disk space for AI models
- WebGPU-capable browser (Chrome 113+, Edge 113+) for Llama

### Installation

```bash
# 1. Install dependencies
npm install
pip install -r requirements.txt

# 2. Download AI models from Hugging Face CDNs (one-time, ~1.5GB)
npm run setup

# 3. Start both servers
npm run start
```

This starts:
- **Vite dev server** at `http://localhost:3000` (frontend)
- **Flask API** at `http://localhost:5000` (PDF redaction backend)

### Alternative: Run Servers Separately

```bash
# Terminal 1: Frontend
npm run dev

# Terminal 2: Backend  
npm run server
# or: python server.py
```

## ğŸ“ Project Structure

```
Zero-Trust-Redactor/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.js              # Application entry point
â”‚   â”œâ”€â”€ style.css            # Styles
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ ai-engine.js     # BERT & Llama model handling
â”‚       â”œâ”€â”€ pdf-processor.js # PDF text extraction
â”‚       â””â”€â”€ redaction-service.js # Backend communication
â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ models/              # Downloaded AI models (after setup)
â”‚   â””â”€â”€ pdf.worker.min.mjs   # PDF.js worker
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ download-models.js   # Model download automation
â”œâ”€â”€ server.py                # Flask backend for PDF redaction
â”œâ”€â”€ index.html               # Main HTML
â”œâ”€â”€ vite.config.js           # Vite configuration
â””â”€â”€ package.json
```

## ğŸ› ï¸ Available Commands

| Command | Description |
|---------|-------------|
| `npm run dev` | Start Vite dev server |
| `npm run server` | Start Python Flask backend |
| `npm run start` | Start both servers concurrently |
| `npm run setup` | Download all AI models (~1.5GB) |
| `npm run setup:bert` | Download only BERT model (~400MB) |
| `npm run setup:llama` | Download only Llama model (~1GB) |
| `npm run build` | Build for production |

## ğŸ”§ Configuration

### Model Paths

Models are stored in `public/models/`:
- BERT NER: `public/models/transformers/Xenova/bert-base-NER/`
- Llama 3.2: `public/models/llama/Llama-3.2-1B-Instruct-q4f16_1-MLC/`

### Vite Proxy

The dev server proxies `/api/*` requests to `http://127.0.0.1:5000` for Flask backend communication.

## ğŸ¯ Usage

1. **Upload a PDF** - Drag & drop or click to browse
2. **Select Mode**:
   - *Fast Mode*: Quick BERT + regex detection
   - *Deep Scan*: Context-aware Llama analysis
3. **Review Targets** - Check detected entities, add/remove as needed
4. **Process** - Click "Process & Redact PDF" to download redacted version

### Manual Entity Addition

Select text in the "Raw Text" panel to manually add items to the redaction list.

## ğŸ”’ Privacy Guarantee

- **No cloud uploads** - All AI inference runs locally; document bytes never leave your device
- **One-time downloads only** - Models and JS libs are fetched during setup/page load from trusted CDNs; no document data is sent
- **Original files untouched** - New redacted copies are created
- **Temporary files cleaned** - Input files are deleted after processing
- **No telemetry** - No analytics or tracking

## ğŸ“¦ Dependencies

### Frontend
- `@xenova/transformers` - BERT NER model
- `@mlc-ai/web-llm` - Llama 3 WebGPU inference
- `pdfjs-dist` - PDF text extraction
- `jspdf` - Report generation
- `vite` - Build tool

### Backend
- `flask` - Web server
- `flask-cors` - CORS handling
- `pymupdf` (fitz) - PDF manipulation & redaction

## ğŸš§ Troubleshooting

### "Models not loading"
Run `npm run setup` to download models. Ensure `public/models/` directory exists.

### "WebGPU not supported"  
Llama 3 requires WebGPU. Use Chrome 113+ or Edge 113+. Fast Mode (BERT) works on all browsers.

### "Server connection failed"
Ensure Python server is running: `npm run server` or `python server.py`

### "PDF worker not found"
The worker should be at `public/pdf.worker.min.mjs`. Re-run `npm run setup` if missing.

## ğŸ“„ License

GNU General Public License v3.0 (GPL-3.0-only). See LICENSE for details.

---

Built for YMYL (Your Money or Your Life) data privacy. Handle financial, medical, and legal documents with confidence.
